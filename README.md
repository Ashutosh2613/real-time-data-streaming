# Real-Time Data Streaming using Apache NiFi, AWS and Snowflake

### Introduction
This project demonstrates a real-time data streaming pipeline using Apache NiFi, AWS services, and Snowflake. The pipeline is designed to simulate the generation of fake data, stream it into AWS S3 using Apache NiFi, and subsequently load it into Snowflake for further processing and storage.

### Architecture Diagram
![Untitled Diagram](https://github.com/user-attachments/assets/b6c6d436-2651-43c9-90ff-2f479ab5895e)

### Architecture Overview
**1. Amazon EC2 :** 
A Docker container is set up on an EC2 instance, running the following services:
* Jupyter Notebook: Used to generate fake data and create CSV files.
* Apache NiFi: Automates the loading of generated CSV files into an S3 bucket.
* Apache ZooKeeper: Coordinates distributed processes.

**2. Amazon S3 :** 
The CSV files generated by Jupyter Notebook are loaded into an S3 bucket through Apache NiFi.

**3. Snowpipe :**  Snowpipe is used to automatically load the data from the S3 bucket into a Snowflake staging/raw table.

**4. Snowflake :** 
* Staging/Raw Table: Temporary storage for raw data.
* Main Table: Stores the latest records, with updates based on a unique key.
* Hist Table: Stores historical data for auditing or analysis.

* Snowflake Stream and Task:
Stream monitors changes in the staging table.
Task automates the movement of data from the staging table to the main and history tables. After the data is transferred, the staging table is truncated to maintain an efficient storage state.



